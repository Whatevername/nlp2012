Algorithm;;Why should you want to know more about algorithms? There are a number of possible reasons: To avoid re-inventing the wheel. For many programming problems there are several simple and elegant algorithms that have been invented, but some of them are better than other in practical situations. These algorithms undergone a tremendous amount of polishing and peer reviews and usually you can be confident in their correctness and efficiency. Sometimes even formal properties of algorithms were discovered. For example, we know that quicksort requires approximately O(n log n) comparisons to sort a file or array, while simpler sorting algorithms the number of comparison is proportional to the square of the number of sorted elements O(n**2).Another fast sorting algorithm mergesort has the same worst case behavior O(n log n) but is stable (meaning that elements with identical keys will stay ordered as they were before the sort). Those formulas mean that for sorting a small number of items simpler algorithms (or shellsort) are better than quicksort and for sorting large number of items mergesort is better as it can utilize natural orderness of the data ( can benefit from the fact that most "real" files are partially sorted; quicksort does not take into account this property; moreover it demonstrates worst case behavior on some nearly sorted files). Partially ordered files probably represent the most important in practice case of sorting that is often overlooked in superficial books. Often sorting is initiated just to put a few new records in proper place of the already sorted file. BTW this is a good litmus test of a quality of any book that covers sorting; if a book does not provide this information you might try to find another. To help to develop your own programs and algorithms. Not all tasks have "off-the-shelf'' algorithms available, so sometimes you need to develop your own or combine several existing. A knowledge of classic algorithms will provide a spectrum of ideas that might help to solve such tasks. To help to understand the behavior and limitations of tools that use a particular algorithm, for example why compression programs cannot compress well any random files. Just to pass a course at the college or university ;-). This is a pretty legitimate goal. Nothing to be ashamed of. And last but not least: to became a better programmer. Programmer without knowledge of algorithms is just an amateur/hacker (which is actually can be defined an amateur obsessed with hardware who spend inordinate time using keyboard instead of thinking about the problem in hand ;-) no matter how well he/she knows a particular programming language. For all these reasons it is useful to have a broad familiarity with a range of algorithms, and to understand what types of problems they can help to solve. This is especially important for open source developers. Algorithms are intrinsically connected with data structures because data structures are dreaming to become elegant algorithms the same way ordinary people are dreaming about Hollywood actors ;-) I would like to note that the value of browsing the WEB in search of algorithms is somewhat questionable :-). One probably will be much better off buying Donald Knuth'sTAOCP and ... disconnecting from the Internet for a couple of months. Note: MIX assembler is not very essential :-). Actually it represent class of computers which became obsolite even before the first volume Knuth book was published (with the release of IBM 360 which took world by storm).You can use Intel assembler if you wish. I used to teach TAOCP on mainframes and IBM assembler was pretty much OK. Knuth probably would be better off using a derivative of S/360 architecture and assembler from the very beginning ;-).Later he updated MIX to MMIX with a new instruction set for use in future volumes, but still existing three volumes use an old one.  There are pages on Internet that have them rewritten in IBM 260 or other more modern instruction set. An extremely important and often overlooked class of algorithms are compiler algorithms (lex analysis, parsing and code generation). They actually represent a very efficient paradigm for solving wide class of tasks similar to compilation of the program. For example complex conversions from one format to another. Also any complex program usually has its own command language and knowledge of compiler algorithms can prevent authors from making typical for amateurs blunders (which are common in open source software). Many authors writing about algorithms try to hide their lack of competence with abuse of mathematical symbolic. Excessive use of mathematics in presentation of algorithms is often counterproductive (verification fiasco can serve as a warning for all future generation; it buried with them such talented authors as E. Dijkstra, David Gries and many others) Also please remember Famous Donald Knuth's citation about correctness proofs: On March 22, 1977, as I was drafting Section 7.1 of The Art of Computer Programming, I read four papers by Peter van Emde Boas that turned out to be more appropriate for Chapter 8 than Chapter 7. I wrote a five-page memo entitled ``Notes on the van Emde Boas construction of priority deques: An instructive use of recursion,'' and sent it to Peter on March 29 (with copies also to Bob Tarjan and John Hopcroft). The final sentence was this: ``Beware of bugs in the above code; I have only proved it correct, not tried it.' 'See also Softpanorama Computer Books Reviews / Algorithms Dr. Nikolai Bezroukov P.S: In order to save the bandwidth for humans (as opposed to robots ;-), previous years of Old News section were converted into separate pages.
Administrator;;A database administrator (short form DBA) is a person responsible for the installation, configuration, upgrade, administration, monitoring and maintenance of databases in an organization. The role includes the development and design of database strategies, system monitoring and improving database performance and capacity, and planning for future expansion requirements. They may also plan, co-ordinate and implement security measures to safeguard the database.
Backend;;A back-end database is a database that is accessed by users indirectly through an external application rather than by application programming stored within the database itself or by low level manipulation of the data (e.g. through SQL commands). A back-end database stores data but does not include end-user application elements such as stored queries, forms, macros or reports.
Bandwidth;;In computer networking and computer science, the words bandwidth, network bandwidth, data bandwidth, or digital bandwidth are terms used to refer to various bit-rate measures, representing the available or consumed data communication resources expressed in bits per second or multiples of it (bit/s, kbit/s, Mbit/s, Gbit/s, etc.). Note that in textbooks on signal processing, wireless communications, modem data transmission, digital communications, electronics, etc., the word 'bandwidth' is used to refer to analog signal bandwidth measured in hertz. The connection is that according to Hartley's law, the digital data rate limit (or channel capacity) of a physical communication link is proportional to its bandwidth in hertz.
Cache;;In computer science, a cache is a component that transparently stores data so that future requests for that data can be served faster. The data that is stored within a cache might be values that have been computed earlier or duplicates of original values that are stored elsewhere. If requested data is contained in the cache (cache hit), this request can be served by simply reading the cache, which is comparatively faster. Otherwise (cache miss), the data has to be recomputed or fetched from its original storage location, which is comparatively slower. Hence, the greater the number of requests that can be served from the cache, the faster the overall system performance becomes. To be cost efficient and to enable an efficient use of data, caches are relatively small. Nevertheless, caches have proven themselves in many areas of computing because access patterns in typical computer applications have locality of reference. References exhibit temporal locality if data is requested again that has been recently requested already. References exhibit spatial locality if data is requested that is physically stored close to data that has been requested already.
Cluster;;A cluster is a small group or bunch of something.
Database;;A database is an organized collection of data. The data are typically organized to model relevant aspects of reality (for example, the availability of rooms in hotels), in a way that supports processes requiring this information (for example, finding a hotel with vacancies). The term database is correctly applied to the data and their supporting data structures, and not to the database management system (DBMS). The database data collection with DBMS is called a database system. The term database system implies that the data are managed to some level of quality (measured in terms of accuracy, availability, usability, and resilience) and this in turn often implies the use of a general-purpose database management system (DBMS). A general-purpose DBMS is typically a complex software system that meets many usage requirements to properly maintain its databases which are often large and complex. The utilization of databases is now so widespread that virtually every technology and product relies on databases and DBMSs for its development and commercialization, or even may have DBMS software embedded in it. Also, organizations and companies, from small to large, depend heavily on databases for their operations. Well known DBMSs include FoxPro, IBM DB2, Linter, Microsoft Access, Microsoft SQL Server, MySQL, Oracle, PostgreSQL and SQLite. A database is not generally portable across different DBMS, but different DBMSs can inter-operate to some degree by using standards like SQL and ODBC together to support a single application built over more than one database. A DBMS also needs to provide effective run-time execution to properly support (e.g., in terms of performance, availability, and security) as many database end-users as needed. A way to classify databases involves the type of their contents, for example: bibliographic, document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. The term database may be narrowed to specify particular aspects of organized collection of data and may refer to the logical database, to the physical database as data content in computer data storage or to many other database sub-definitions.
Domain;;An application domain is a mechanism (similar to a process in an operating system) used within a Common Language Infrastructure (CLI) to isolate executed software applications from one another so that they do not affect each other. Each application domain has its own virtual address space which scopes the resources for the application domain using that address space.
Encryption;;In cryptography, encryption is the process of encoding messages (or information) in such a way that eavesdroppers or hackers cannot read it, but that authorized parties can[1]:374. In an encryption scheme, the message or information (referred to as plaintext) is encrypted using an encryption algorithm, turning it into an unreadable ciphertext (ibid.). This is usually done with the use of an encryption key, which specifies how the message is to be encoded. Any adversary that can see the ciphertext, should not be able to determine anything about the original message. An authorized party, however, is able to decode the ciphertext using a decryption algorithm, that usually requires a secret decryption key, that adversaries do not have access to. For technical reasons, an encryption scheme usually needs a key-generation algorithm, to randomly produce keys. There are two basic types of encryption schemes private-key encryption and public-key encryption. In private-key schemes, the encryption and decryption keys are the same. Thus communicating parties must agree on a secret key before they wish to communicate. By contrast, in public-key schemes, the encryption key is public: that is, anyone (friend or foe) has access to the encryption key, and can encrypt messages. However only the receiving party has access to the decryption key and thus is the only one capable of reading the encrypted messages. Public-key encryption is a relatively recent invention: historically, all encryption schemes have been private-key schemes. Encryption has long been used by militaries and governments to facilitate secret communication. It is now commonly used in protecting information within many kinds of civilian systems. For example, the Computer Security Institute reported that in 2007, 71% of companies surveyed utilized encryption for some of their data in transit, and 53% utilized encryption for some of their data in storage. Encryption can be used to protect data "at rest", such as files on computers and storage devices (e.g. USB flash drives). In recent years there have been numerous reports of confidential data such as customers' personal records being exposed through loss or theft of laptops or backup drives. Encrypting such files at rest helps protect them should physical security measures fail. Digital rights management systems which prevent unauthorized use or reproduction of copyrighted material and protect software against reverse engineering (see also copy protection) are another somewhat different example of using encryption on data at rest.[citation needed] Encryption is also used to protect data in transit, for example data being transferred via networks (e.g. the Internet, e-commerce), mobile telephones, wireless microphones, wireless intercom systems, Bluetooth devices and bank automatic teller machines. There have been numerous reports of data in transit being intercepted in recent years. Encrypting data in transit also helps to secure it as it is often difficult to physically secure all access to networks.[citation needed] Encryption, by itself, can protect the confidentiality of messages, but other techniques are still needed to protect the integrity and authenticity of a message; for example, verification of a message authentication code (MAC) or a digital signature. Standards and cryptographic software and hardware to perform encryption are widely available, but successfully using encryption to ensure security may be a challenging problem. A single slip-up in system design or execution can allow successful attacks. Sometimes an adversary can obtain unencrypted information without directly undoing the encryption. See, e.g., traffic analysis, TEMPEST, or Trojan horse.[citation needed] One of the earliest public key encryption applications was called Pretty Good Privacy (PGP). It was written in 1991 by Phil Zimmermann and was purchased by Symantec in 2010. Digital signature and encryption must be applied at message creation time (i.e. on the same device it has been composed) to avoid tampering. Otherwise any node between the sender and the encryption agent could potentially tamper it. It should be noted that encrypting at the time of creation only adds security if the encryption device itself has not been tampered with.
Ethernet;;Ethernet is a family of computer networking technologies for local area networks (LANs). Ethernet was commercially introduced in 1980 and standardized in 1985 as IEEE 802.3. Ethernet has largely replaced competing wired LAN technologies. The Ethernet standards comprise several wiring and signaling variants of the OSI physical layer in use with Ethernet. The original 10BASE5 Ethernet used coaxial cable as a shared medium. Later the coaxial cables were replaced by twisted pair and fiber optic links in conjunction with hubs or switches. Data rates were periodically increased from the original 10 megabits per second, to 100 gigabits per second. Systems communicating over Ethernet divide a stream of data into shorter pieces called frames. Each frame contains source and destination addresses and error-checking data so that damaged data can be detected and re-transmitted. As per the OSI model Ethernet provides services up to and including the data link layer. Since its commercial release, Ethernet has retained a good degree of compatibility. Features such as the 48-bit MAC address and Ethernet frame format have influenced other networking protocols.
Firewall;;A firewall can either be software-based or hardware-based and is used to help keep a network secure. Its primary objective is to control the incoming and outgoing network traffic by analyzing the data packets and determining whether it should be allowed through or not, based on a predetermined rule set. A network's firewall builds a bridge between an internal network that is assumed to be secure and trusted, and another network, usually an external (inter)network, such as the Internet, that is not assumed to be secure and trusted. Many personal computer operating systems include software-based firewalls to protect against threats from the public Internet. Many routers that pass data between networks contain firewall components and, conversely, many firewalls can perform basic routing functions.
FTP;;File Transfer Protocol (FTP) is a standard network protocol used to transfer files from one host or to another host over a TCP-based network, such as the Internet. FTP is built on a client-server architecture and uses separate control and data connections between the client and the server. FTP users may authenticate themselves using a clear-text sign-in protocol, normally in the form of a username and password, but can connect anonymously if the server is configured to allow it. For secure transmission that hides (encrypts) the username and password, and encrypts the content, FTP is often secured with SSL/TLS ("FTPS"). SSH File Transfer Protocol ("SFTP") is sometimes also used instead. The first FTP client applications were command-line applications developed before operating systems had graphical user interfaces, and are still shipped with most Windows, Unix, and Linux operating systems. Dozens of FTP clients and automation utilities have since been developed for desktops, servers, mobile devices, and hardware, and FTP has been incorporated into hundreds of productivity applications, such as Web page editors.
Gateway;;Gateway is a phrase used by webmasters and search engine optimizers to describe a webpage designed to attract visitors and search engines to a particular website. A typical gateway page is small, simple and highly optimized. Its primary goal is to attract visitors searching for relevant key words or phrases, and provide hyperlinks to pages within the website.
Gigahertz;;Definition: In wireless communications, the term "Hz" (pronounced Hertz after the name of 19th century scientist Heinrich Hertz) refers to the transmission frequency of radio signals in cycles per second. 1 MHz (Megahertz) equals one million cycles per second, while 1 GHz (Gigahertz) equals one billion cycles per second or 1000 MHz. Bluetooth and WiFi networks signal in the ranges of 2.4 GHz or 5 GHz. These are bands of radio frequency open for public communication (i.e., unregulated) in most countries. WiMax wireless networks use a much larger range of GHz frequencies. A higher frequency such as 5 GHz requires more power to penetrate obstructions than a lower frequency like 2.4 GHz.
Hashtag;;Hashtags are words or phrases prefixed with the symbol #, a form of metadata tag. Also, short messages on microblogging social networking services such as Twitter, Tout, identi.ca or Google+ may be tagged by including one or more with multiple words concatenated, e.g.: #Wikipedia is an #encyclopedia Searching for the string #Wikipedia will cause this word to appear in the search engine results. Such tags are case-insensitive, with CamelCase often used for readability.
Hyperlink;;In computing, a hyperlink (or link) is a reference to data that the reader can directly follow, or that is followed automatically. A hyperlink points to a whole document or to a specific element within a document. Hypertext is text with hyperlinks. A software system for viewing and creating hypertext is a hypertext system, and to create a hyperlink is to hyperlink (or simply to link). A user following hyperlinks is said to navigate or browse the hypertext. A hyperlink has an anchor, which is the location within a certain type of a document from which the hyperlink can be followed only from the homepage; the document containing a hyperlink is known as its source code document. For example, in an online reference work such as Wikipedia, many words and terms in the text are hyperlinked to definitions of those terms. Hyperlinks are often used to implement reference mechanisms, such as tables of contents, footnotes, bibliographies, indexes, letters, and glossaries. In some hypertext, hyperlinks can be bidirectional: they can be followed in two directions, so both ends act as anchors and as targets. More complex arrangements exist, such as many-to-many links. The effect of following a hyperlink may vary with the hypertext system and may sometimes depend on the link itself; for instance, on the World Wide Web, most hyperlinks cause the target document to replace the document being displayed, but some are marked to cause the target document to open in a new window. Another possibility is transclusion, for which the link target is a document fragment that replaces the link anchor within the source document. Not only persons browsing the document follow hyperlinks; they may also be followed automatically by programs. A program that traverses the hypertext, following each hyperlink and gathering all the retrieved documents is known as a Web spider or crawler.
Install;; Prebuilt Packages for Linux and BSD Most Linux distributions and BSD variants have Nginx in the usual package repositories and they can be installed via whatever method is normally used to install software (apt-get on Debian, emerge on Gentoo, ports on FreeBSD, etc). Be aware that these packages are often somewhat out-of-date. If you want the latest features and bugfixes, it's recommended to build from source or use packages directly from nginx.org.
IT;;Information technology (IT) is concerned with the development, management, and use of computer-based information systems. Humans have been storing, retrieving, manipulating and communicating information since the Sumerians in Mesopotamia developed writing in about 3000 BC, but the term "information technology" in its modern sense first appeared in a 1958 article published in the Harvard Business Review; authors Leavitt and Whisler commented that "the new technology does not yet have a single established name. We shall call it information technology (IT)." Based on the storage and processing technology employed, it is possible to distinguish four distinct phases of IT development: pre-mechanical (3000 BC – 1450 AD), mechanical (1450–1840), electromechanical (1840–1940) and electronic.[1] This article focuses on the latter of those periods, which began in about 1940.
Java;;Java is a programming language originally developed by James Gosling at Sun Microsystems (which has since merged into Oracle Corporation) and released in 1995 as a core component of Sun Microsystems' Java platform. The language derives much of its syntax from C and C++, but it has fewer low-level facilities than either of them. Java applications are typically compiled to bytecode (class file) that can run on any Java virtual machine (JVM) regardless of computer architecture. Java is a general-purpose, concurrent, class-based, object-oriented language that is specifically designed to have as few implementation dependencies as possible. It is intended to let application developers "write once, run anywhere" (WORA), meaning that code that runs on one platform does not need to be recompiled to run on another. Java is as of 2012 one of the most popular programming languages in use, particularly for client-server web applications, with a reported 10 million users. The original and reference implementation Java compilers, virtual machines, and class libraries were developed by Sun from 1991 and first released in 1995. As of May 2007, in compliance with the specifications of the Java Community Process, Sun relicensed most of its Java technologies under the GNU General Public License. Others have also developed alternative implementations of these Sun technologies, such as the GNU Compiler for Java and GNU Classpath.
Joystick;;A joystick is an input device consisting of a stick that pivots on a base and reports its angle or direction to the device it is controlling. A joystick, also known as the control column, is the principal control device in the cockpit of many civilian and military aircraft, either as a center stick or side-stick. It often has supplementary switches to control various aspects of the aircraft's flight. Joysticks are often used to control video games, and usually have one or more push-buttons whose state can also be read by the computer. A popular variation of the joystick used on modern video game consoles is the analog stick. Joysticks are also used for controlling machines such as cranes, trucks, underwater unmanned vehicles, wheelchairs, surveillance cameras and zero turning radius lawn mowers. Miniature finger-operated joysticks have been adopted as input devices for smaller electronic equipment such as mobile phones.
Kernel;;In computing, the kernel is the main component of most computer operating systems; it is a bridge between applications and the actual data processing done at the hardware level. The kernel's responsibilities include managing the system's resources (the communication between hardware and software components). Usually, as a basic component of an operating system, a kernel can provide the lowest-level abstraction layer for the resources (especially processors and I/O devices) that application software must control to perform its function. It typically makes these facilities available to application processes through inter-process communication mechanisms and system calls. Operating system tasks are done differently by different kernels, depending on their design and implementation. While monolithic kernels execute all the operating system code in the same address space to increase the performance of the system, microkernels run most of the operating system services in user space as servers, aiming to improve maintainability and modularity of the operating system. A range of possibilities exists between these two extremes.
Keyboard;;In computing, a keyboard is a typewriter-style device, which uses an arrangement of buttons or keys, to act as mechanical levers or electronic switches. Following the decline of punch cards and paper tape, interaction via teleprinter-style keyboards became the main input device for computers. A keyboard typically has characters engraved or printed on the keys and each press of a key typically corresponds to a single written symbol. However, to produce some symbols requires pressing and holding several keys simultaneously or in sequence. While most keyboard keys produce letters, numbers or signs (characters), other keys or simultaneous key presses can produce actions or computer commands. Despite the development of alternative input devices, such as the mouse, touchscreen, pen devices, character recognition and voice recognition, the keyboard remains the most commonly used and most versatile device used for direct (human) input into computers. In normal usage, the keyboard is used to type text and numbers into a word processor, text editor or other programs. In a modern computer, the interpretation of key presses is generally left to the software. A computer keyboard distinguishes each physical key from every other and reports all key presses to the controlling software. Keyboards are also used for computer gaming, either with regular keyboards or by using keyboards with special gaming features, which can expedite frequently used keystroke combinations. A keyboard is also used to give commands to the operating system of a computer, such as Windows' Control-Alt-Delete combination, which brings up a task window or shuts down the machine. Keyboards are the only way to enter commands on a command-line interface.
Localhost;;In computer networking, localhost (meaning this computer) is the standard hostname given to the address of the loopback network interface. Simply put, when a user sends data to localhost, they will receive their own data. This mechanism is useful for programmers to test their software during development. The name is also a reserved top-level domain name (cf. .localhost), set aside to avoid confusion with the narrower definition as a hostname. On modern computer systems, localhost as a hostname translates to an IPv4 address in the 127.0.0.0/8 (loopback) net block, usually 127.0.0.1, or ::1 in IPv6. Localhost is specified where one would otherwise use the hostname of a computer. For example, directing a web browser installed on a system running an HTTP server to http://localhost will display the home page of the web site installed on that system, provided the server is configured to service the loopback interface. Using the loopback interface also bypasses local network interface hardware. Connecting to locally hosted network services such as a computer game server using loopback addresses puts less of a load on network resources. The Internet Engineering Task Force (IETF) Internet Standard document STD-2 series (e.g., RFC 1700) reserved the 127.0.0.0/8 address block for loopback purposes. until such definitions were updated exclusively through the Internet Assigned Numbers Authority (IANA) website. A later IETF document, Special-Use IPv4 Addresses (RFC 3330) describes the usage of the IPv4 address block 127.0.0.0/8 for loopback purposes. It is therefore excluded from assignment by a Regional Internet Registry or IANA. Lastly, RFC 3330 has been superseded by the newer RFC 5735. For IPv4 communications, the virtual loopback interface of a computer system is normally assigned the address 127.0.0.1 with subnet mask 255.0.0.0. Depending on the specific operating system in use (notably in Linux and Microsoft Windows) and the routing mechanisms installed, this populates the routing table of the local system with an entry so that packets destined to any address from the 127.0.0.0/8 block would be routed internally to the network loopback device. In IPv6, on the other hand, the loopback routing prefix ::1/128 consists of only one address ::1 (0:0:0:0:0:0:0:1 in full notation, the address with a one at its least significant bit and zero otherwise) is explicitly defined as the loopback address, though additional addresses may be assigned as needed to the loopback interface by the host administrator. Any IP datagram with a source or destination address set to a loopback address must not appear outside of a computing system, or be routed by any routing device. Packets received on an interface with a loopback destination address must be dropped. One notable exception to the use of the 127/8 network addresses is their use in Multiprotocol Label Switching (MPLS) traceroute error detection techniques (RFC 4379) in which their property of not being routable provides a convenient means to avoid delivery of faulty packets to end users.
Linux;;Linux is a Unix-like computer operating system assembled under the model of free and open source software development and distribution. The defining component of Linux is the Linux kernel, an operating system kernel first released 5 October 1991 by Linus Torvalds. Linux was originally developed as a free operating system for Intel x86-based personal computers. It has since been ported to more computer hardware platforms than any other operating system. It is a leading operating system on servers and other big iron systems such as mainframe computers and supercomputers: more than 90% of today's 500 fastest supercomputers run some variant of Linux, including the 10 fastest. Linux also runs on embedded systems (devices where the operating system is typically built into the firmware and highly tailored to the system) such as mobile phones, tablet computers, network routers, televisions and video game consoles; the Android system in wide use on mobile devices is built on the Linux kernel. The development of Linux is one of the most prominent examples of free and open source software collaboration: the underlying source code may be used, modified, and distributed—commercially or non-commercially—by anyone under licenses such as the GNU General Public License. Typically Linux is packaged in a format known as a Linux distribution for desktop and server use. Some popular mainstream Linux distributions include Debian (and its derivatives such as Ubuntu), Fedora and openSUSE. Linux distributions include the Linux kernel, supporting utilities and libraries and usually a large amount of application software to fulfill the distribution's intended use. A distribution oriented toward desktop use will typically include the X Window System and an accompanying desktop environment such as GNOME or KDE Plasma. Some such distributions may include a less resource intensive desktop such as LXDE or Xfce for use on older or less powerful computers. A distribution intended to run as a server may omit all graphical environments from the standard install and instead include other software such as the Apache HTTP Server and an SSH server such as OpenSSH. Because Linux is freely redistributable, anyone may create a distribution for any intended use. Applications commonly used with desktop Linux systems include the Mozilla Firefox web browser, the LibreOffice office application suite, and the GIMP image editor. Since the C compiler Linux is written with and the main supporting user space system tools and libraries originated in the GNU Project, initiated in 1983 by Richard Stallman, the Free Software Foundation prefers the name GNU/Linux.
Malware;;Malware, short for malicious software, is software used or created to disrupt computer operation, gather sensitive information, or gain access to private computer systems. It can appear in the form of code, scripts, active content, and other software. 'Malware' is a general term used to refer to a variety of forms of hostile or intrusive software. Malware includes computer viruses, worms, trojan horses, spyware, adware, and other malicious programs. In law, malware is sometimes known as a computer contaminant, as in the legal codes of several U.S. states. Malware is not the same as defective software, which is software that has a legitimate purpose but contains harmful bugs that were not corrected before release. However, some malware is disguised as genuine software, and may come from an official company website. An example of this is software used for harmless purposes that is packed with additional tracking software that gathers marketing statistics. Malware has caused the rise in use of protective software types such as anti virus, anti-malware, and firewalls. Each of these are commonly used by personal users and corporate networks in order to stop the unauthorized access by other computer users, as well as the automated spread of malicious scripts and software.
Megabyte;;The megabyte (abbreviated as Mbyte or MB) is a multiple of the unit byte for digital information storage or transmission with three different values depending on context: 1048576 bytes (220) generally for computer memory; and one million bytes (106, see prefix mega-) generally for computer storage. In rare cases, it is used to mean 1000×1024 (1024000) bytes. The IEEE Standards Board has confirmed that mega- means 1000000, with exceptions allowed for the base-two meaning.
Newsgroup;;A Usenet newsgroup is a repository usually within the Usenet system, for messages posted from many users in different locations. The term may be confusing to some, because it is in fact a discussion group. Newsgroups are technically distinct from, but functionally similar to, discussion forums on the World Wide Web. Newsreader software is used to read newsgroups. Newsgroups are much like the public message boards on old bulletin board systems. For those readers not familiar with this concept, envision an electronic version of the corkboard in the entrance of your local grocery store. Despite the advent of file-sharing technologies such as BitTorrent, as well as the increased use of blogs, formal discussion forums, and social networking sites, coupled with a growing number of service providers blocking access to Usenet (see main article), newsgroups continue to be widely used
NTFS;;NTFS (New Technology File System) is a proprietary file system developed by Microsoft Corporation for its Windows line of operating systems, beginning with Windows NT 3.1 and Windows 2000, including Windows XP, Windows Server 2003, and all their successors to date. NTFS supersedes the FAT file system as the preferred file system for Microsoft’s Windows operating systems. NTFS has several improvements over FAT and HPFS (High Performance File System), such as improved support for metadata, and the use of advanced data structures to improve performance, reliability, and disk space utilization, plus additional extensions, such as security access control lists (ACL) and file system journaling.
OpenGL;;OpenGL is a cross-language[disambiguation needed], multi-platform API for rendering 2D and 3D computer graphics. The API is typically used to interact with a GPU, to achieve hardware-accelerated rendering. OpenGL was developed by Silicon Graphics Inc. in 1992 and is widely used in CAD, virtual reality, scientific visualization, information visualization, flight simulation, and video games. OpenGL is managed by the non-profit technology consortium Khronos Group.
Overclocking;;Overclocking is the process of making a computer or component operate faster than the clock frequency specified by the manufacturer by modifying system parameters. One of the most important techniques is running at a higher clock rate (more clock cycles per second; hence the name "overclocking"), but other parameters, such as CPU multiplier and memory timings, can also be changed and would be considered to be overclocking. Operating voltages may also be changed (increased), which can increase the speed at which operation remains stable. Most overclocking techniques increase power consumption, generating more heat, which must be dispersed if the chip is to remain operational. The purpose of overclocking is to increase the operating speed of given hardware. The trade-offs are an increase in power consumption and fan noise, the system can become unstable if the equipment is overclocked too much, and the risk of damage due to excessive overvoltage or heat generation. In extreme cases costly and complex cooling (e.g., water-cooling) is required.
Ping;;Ping is a computer network administration utility used to test the reachability of a host on an Internet Protocol (IP) network and to measure the round-trip time for messages sent from the originating host to a destination computer. The name comes from active sonar terminology which sends a pulse of sound and listens for the echo to detect objects underwater. Ping operates by sending Internet Control Message Protocol (ICMP) echo request packets to the target host and waiting for an ICMP response. In the process it measures the time from transmission to reception (round-trip time) and records any packet loss. The results of the test are printed in the form of a statistical summary of the response packets received, including the minimum, maximum, and the mean round-trip times, and sometimes the standard deviation of the mean. Depending on the implementation, the ping command can be run with various command line switches to enable special operational modes. Example options include: specifying the packet size used as the probe, automatic repeated operation for sending a specified count of probes, and time stamping. Ping may be abused as a simple form of denial-of-service attack in the form of a ping flood, in which the attacker overwhelms the victim with ICMP echo request packets.
Pixel;;In digital imaging, a pixel, or pel,(picture element) is a physical point in a raster image, or the smallest addressable element in a display device; so it is the smallest controllable element of a picture represented on the screen. The address of a pixel corresponds to its physical coordinates. LCD pixels are manufactured in a two-dimensional grid, and are often represented using dots or squares, but CRT pixels correspond to their timing mechanisms and sweep rates. Each pixel is a sample of an original image; more samples typically provide more accurate representations of the original. The intensity of each pixel is variable. In color image systems, a color is typically represented by three or four component intensities such as red, green, and blue, or cyan, magenta, yellow, and black. In some contexts (such as descriptions of camera sensors), the term pixel is used to refer to a single scalar element of a multi-component representation (more precisely called a photosite in the camera sensor context, although the neologism sensel is sometimes used to describe the elements of a digital camera's sensor), while in others the term may refer to the entire set of such component intensities for a spatial position. In color systems that use chroma subsampling, the multi-component concept of a pixel can become difficult to apply, since the intensity measures for the different color components correspond to different spatial areas in such a representation. The word pixel is based on a contraction of pix ("pictures") and el (for "element"); similar formations with el  for "element" include the words voxel and texel.
Query;;Query languages are computer languages used to make queries into databases and information systems. Broadly, query languages can be classified according to whether they are database query languages or information retrieval query languages. The difference is that a database query language attempts to give factual answers to factual questions, while an information retrieval query language attempts to find documents containing information that is relevant to an area of inquiry.
Queue;;In computer science, a queue ( /'kju?/ kew) is a particular kind of abstract data type or collection in which the entities in the collection are kept in order and the principal (or only) operations on the collection are the addition of entities to the rear terminal position and removal of entities from the front terminal position. This makes the queue a First-In-First-Out (FIFO) data structure. In a FIFO data structure, the first element added to the queue will be the first one to be removed. This is equivalent to the requirement that once an element is added, all elements that were added before have to be removed before the new element can be invoked. A queue is an example of a linear data structure. Queues provide services in computer science, transport, and operations research where various entities such as data, objects, persons, or events are stored and held to be processed later. In these contexts, the queue performs the function of a buffer. Queues are common in computer programs, where they are implemented as data structures coupled with access routines, as an abstract data structure or in object-oriented languages as classes. Common implementations are circular buffers and linked lists.
Router;;A router is a device that forwards data packets between computer networks, creating an overlay internetwork. A router is connected to two or more data lines from different networks. When a data packet comes in one of the lines, the router reads the address information in the packet to determine its ultimate destination. Then, using information in its routing table or routing policy, it directs the packet to the next network on its journey. Routers perform the "traffic directing" functions on the Internet. A data packet is typically forwarded from one router to another through the networks that constitute the internetwork until it gets to its destination node. The most familiar type of routers are home and small office routers that simply pass data, such as web pages and email, between the home computers and the owner's cable or DSL modem, which connects to the Internet through an ISP. More sophisticated routers, such as enterprise routers, connect large business or ISP networks up to the powerful core routers that forward data at high speed along the optical fiber lines of the Internet backbone. Though routers are typically dedicated hardware devices, use of software-based routers has grown increasingly common.
Runtime;;In computer science, run time, run-time, runtime, or execution time is the time during which a program is running (executing), in contrast to other phases of a program's lifecycle such as compile time, link time, load time, etc. A run-time error is detected after or during the execution of a program, whereas a compile-time error is detected by the compiler before the program is ever executed. Type checking, storage allocation, code generation, and code optimization are typically done at compile time, but may be done at run time depending on the particular language and compiler.
Scrolling;;In computer displays, filmmaking, television production, and other kinetic displays, scrolling is sliding text, images or video across a monitor or display, vertically or horizontally. "Scrolling", as such, does not change the layout of the text or pictures, but moves (pans or tilts) the user's view across what is apparently a larger image that is not wholly seen. A common television and movie special effect is to scroll credits, while leaving the background stationary. Scrolling may take place completely without user intervention (as in film credits) or, on an interactive device, be triggered by touchscreen or computer mouse motion or a keypress and continue without further intervention until a further user action, or be entirely controlled by input devices. Scrolling may take place in discrete increments (perhaps one or a few lines of text at a time), or continuously (smooth scrolling). Frame rate is the speed at which an entire image is redisplayed. It is related to scrolling in that changes to text and image position can only happen as often as the image can be redisplayed. When frame rate is a limiting factor, one smooth scrolling technique is to blur images during movement that would otherwise appear to "jump". The term scrolling is also used for a type of misbehavior in an online chat room whereby one person forces the screens of others in a chat to scroll by inserting much noise or special control character.
Server;;In most common use, a server is a physical computer (a computer hardware system) dedicated to run one or more services (as a host), to serve the needs of the users of other computers on the network. Depending on the computing service that it offers it could be a database server, file server, mail server, print server, web server, gaming server, or some other kind of server. In the context of client-server architecture, a server is a computer program running to serve the requests of other programs, the "clients". Thus, the "server" performs some computational task on behalf of "clients". The clients either run on the same computer or connect through the network. In the context of Internet Protocol (IP) networking, a server is a program that operates as a socket listener. Servers often provide essential services across a network, either to private users inside a large organization or to public users via the Internet.
Toolbar;;In a graphical user interface, on a computer monitor, a toolbar is a GUI widget on which on-screen buttons, icons, menus, or other input or output elements are placed. Toolbars are seen in many types of applications such as office suites, graphics editors, and web browsers. Toolbars are usually distinguished from palettes by their integration into the edges of the screen or larger windows, which results in wasted space if too many underpopulated bars are stacked atop each other (especially horizontal bars on a landscape oriented display) or interface inefficiency if overloaded bars are placed on small windows.
Touchscreen;;A touchscreen is an electronic visual display that can detect the presence and location of a touch within the display area. The term generally refers to touching the display of the device with a finger or hand. Touchscreens can also sense other passive objects, such as a stylus. Touchscreens are common in devices such as game consoles, all-in-one computers, tablet computers, and smartphones. The touchscreen has two main attributes. First, it enables one to interact directly with what is displayed, rather than indirectly with a pointer controlled by a mouse or touchpad. Secondly, it lets one do so without requiring any intermediate device that would need to be held in the hand (other than a stylus, which is optional for most modern touchscreens). Such displays can be attached to computers, or to networks as terminals. They also play a prominent role in the design of digital appliances such as the personal digital assistant (PDA), satellite navigation devices, mobile phones, and video games. The popularity of smartphones, tablet computers and many types of information appliances is driving the demand and acceptance of common touchscreens for portable and functional electronics. With a display of a simple smooth surface, and direct interaction without any hardware (keyboard or mouse) between the user and content, fewer accessories are required. Touchscreens are popular in the medical field, and in heavy industry, as well as kiosks such as museum displays or room automation, where keyboard and mouse systems do not allow a suitably intuitive, rapid, or accurate interaction by the user with the display's content. Historically, the touchscreen sensor and its accompanying controller-based firmware have been made available by a wide array of after-market system integrators, and not by display, chip, or motherboard manufacturers. Display manufacturers and chip manufacturers worldwide have acknowledged the trend toward acceptance of touchscreens as a highly desirable user interface component and have begun to integrate touchscreens into the fundamental design of their products.
URL;;In computing, a uniform resource locator (URL) (originally called universal resource locator) is a specific character string. A URL is technically a type of uniform resource identifier (URI) but in many technical documents and verbal discussions URL is often used as a synonym for URI.
Username;;A user is an agent, either a human agent (end-user) or software agent, who uses a computer or network service. A user often has a user account and is identified by a username (also user name). Other terms for username include login name, screen name (also screenname), nickname (also nick), or handle, which is derived from the identical Citizen's Band radio term. Users are also widely characterized as the class of people that use a system without complete technical expertise required to understand the system fully. In hacker-related contexts, such users are also divided into lusers and power users. In projects in which the actor of the system is another system or a software agent, it is quite possible that there is no end-user for the system. In this case, the end-users for the system would be indirect end-users.
VoIP;;Voice over IP (VoIP, or voice over Internet Protocol) commonly refers to the communication protocols, technologies, methodologies, and transmission techniques involved in the delivery of voice communications and multimedia sessions over Internet Protocol (IP) networks, such as the Internet. Other terms commonly associated with VoIP are IP telephony, Internet telephony, voice over broadband (VoBB), broadband telephony, IP communications, and broadband phone. Internet telephony refers to communications services —voice, fax, SMS, and/or voice-messaging applications— that are transported via the Internet, rather than the public switched telephone network (PSTN). The steps involved in originating a VoIP telephone call are signaling and media channel setup, digitization of the analog voice signal, encoding, packetization, and transmission as Internet Protocol (IP) packets over a packet-switched network. On the receiving side, similar steps (usually in the reverse order) such as reception of the IP packets, decoding of the packets and digital-to-analog conversion reproduce the original voice stream. Even though IP Telephony and VoIP are terms that are used interchangeably, they are actually different; IP telephony has to do with digital telephony systems that use IP protocols for voice communication, while VoIP is actually a subset of IP Telephony. VoIP is a technology used by IP telephony as a means of transporting phone calls. Early providers of voice over IP services offered business models (and technical solutions) that mirrored the architecture of the legacy telephone network. Second generation providers, such as Skype have built closed networks for private user bases, offering the benefit of free calls and convenience, while denying their users the ability to call out to other networks. This has severely limited the ability of users to mix-and-match third-party hardware and software. Third generation providers, such as Google Talk have adopted the concept of Federated VoIP – which is a complete departure from the architecture of the legacy networks. These solutions typically allow arbitrary and dynamic interconnection between any two domains on the Internet whenever a user wishes to place a call.
VPN;;A virtual private network (VPN) is a technology for using the Internet or another intermediate network to connect computers to isolated remote computer networks that would otherwise be inaccessible. A VPN provides varying levels of security so that traffic sent through the VPN connection stays isolated from other computers on the intermediate network, either through the use of a dedicated connection from one "end" of the VPN to the other, or through encryption. VPNs can connect individual users to a remote network or connect multiple networks together. For example, users may use a VPN to connect to their work computer terminal from home and access their email, files, images, etc. Through VPNs, users are able to access resources on remote networks, such as files, printers, databases, or internal websites. VPN remote users get the impression of being directly connected to the central network via a point-to-point link.
Webcam;;A webcam is a video camera that feeds its images in real time to a computer or computer network, often via USB, ethernet, or Wi-Fi. Their most popular use is the establishment of video links, permitting computers to act as videophones or videoconference stations. The common use as a video camera for the World Wide Web gave the webcam its name. Other popular uses include security surveillance, computer vision, video broadcasting, and for recording social videos. Webcams are known for their low manufacturing cost and flexibility, making them the lowest cost form of videotelephony. They have also become a source of security and privacy issues, as some built-in webcams can be remotely activated via spyware.
Wireless;;Wireless telecommunications refers to the transfer of information between two or more points that are not physically connected. Distances can be short, such as a few metres for television remote control, or as far as thousands or even millions of kilometres for deep-space radio communications. It encompasses various types of fixed, mobile, and portable applications, including two-way radios, cellular telephones, personal digital assistants (PDAs), and wireless networking. Other examples of wireless technology include GPS units, garage door openers, wireless computer mice, keyboards and headsets, headphones, radio receivers, satellite television, broadcast television and cordless telephones.
X86;;x86 is a series of computer microprocessor instruction set architectures based on the Intel 8086 CPU. The 8086 was introduced during 1978 as a fully 16-bit extension of Intel's 8-bit based 8080 microprocessor and also introduced memory segmentation to overcome the 16-bit addressing barrier of such designs. The term x86 derived from the fact that early successors to the 8086 also had names ending with "86". Many additions and extensions have been added to the x86 instruction set over the years, almost consistently with full backward compatibility. The architecture has been implemented in processors from Intel, Cyrix, Advanced Micro Devices, VIA, and many other companies. The term is not synonymous with IBM PC compatibility as this implies a multitude of other computer hardware; embedded systems as well as general-purpose computers used x86 chips before the PC-compatible market started, some of them before the IBM PC itself. As the term became common after the introduction of the 80386, it usually implies binary compatibility with the 32-bit instruction set of the 80386. This may sometimes be emphasized as x86-32 or x32 to distinguish it either from the original 16-bit "x86-16" or from the 64-bit x86-64. Although most x86 processors used in new personal computers and servers have 64-bit capabilities, to avoid compatibility problems with older computers or systems, the term x86-64 (or x64) is often used to denote 64-bit software, with the term x86 implying only 32-bit. Although the 8086 was primarily developed for embedded systems and small single-user computers, largely as a response to the successful 8080-compatible Zilog Z80, the x86 line soon grew in features and processing power. Today, x86 is ubiquitous in both stationary and portable personal computers and has replaced midrange computers and Reduced instruction set computer (RISC) based processors in a majority of servers and workstations as well. A large amount of software, including operating systems (OSs) such as DOS, Windows, Linux, BSD, Solaris, and Mac OS X functions with x86-based hardware.
XHTML;;XHTML (Extensible HyperText Markup Language) is a family of XML markup languages that mirror or extend versions of the widely used Hypertext Markup Language (HTML), the language in which web pages are written. While HTML (prior to HTML5) was defined as an application of Standard Generalized Markup Language (SGML), a very flexible markup language framework, XHTML is an application of XML, a more restrictive subset of SGML. Because XHTML documents need to be well-formed, they can be parsed using standard XML parsers—unlike HTML, which requires a lenient HTML-specific parser. XHTML 1.0 became a World Wide Web Consortium (W3C) Recommendation on January 26, 2000. XHTML 1.1 became a W3C Recommendation on May 31, 2001. XHTML5 is undergoing development as of September 2009, as part of the HTML5 specification.
Yobibyte;;The yobibyte is a standards-based binary multiple (prefix yobi, symbol Yi) of the byte, a unit of digital information storage. The yobibyte unit symbol is YiB. 1 yobibyte = 280 bytes = 1208925819614629174706176bytes = 1024 zebibytes. The yobibyte is closely related to the yottabyte, which is defined as 1024 bytes = 1000000000000000000000000bytes (see binary prefix). The zebi and yobi prefixes were originally not part of the system of binary prefixes, but were added by the International Electrotechnical Commission in August 2005.
Zip;;Zip is a file format used for data compression and archiving. A zip file contains one or more files that have been compressed, to reduce file size, or stored as is. The zip file format permits a number of compression algorithms. The format was originally created in 1989 by Phil Katz, and was first implemented in PKWARE's PKZIP utility, as a replacement for the previous ARC compression format by Thom Henderson. The zip format is now supported by many software utilities other than PKZIP. Microsoft has included built-in zip support (under the name "compressed folders") in versions of Microsoft Windows since 1998. Apple has included built-in zip support in Mac OS X 10.3 (via BOMArchiveHelper, now Archive Utility) and later. Zip files generally use the file extensions ".zip" or ".ZIP" and the MIME media type application/zip. Zip is used as a base file format by many programs, usually under a different name. Zip files are often represented by a document or other object prominently featuring a zipper.
